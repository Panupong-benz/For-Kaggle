# @package _global_
# Full LoRA Configuration for SAM3
# Maximum capacity - all components
# AI Research Group - KMUTT

defaults:
  - lora_base
  - _self_

# Override LoRA settings for full configuration
lora:
  rank: 16                         # Higher rank for more capacity
  alpha: 32
  dropout: 0.1                     # Some dropout for regularization

  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "out_proj"
    - "fc1"                        # MLP first layer
    - "fc2"                        # MLP second layer

  # Apply to ALL components
  apply_to_vision_encoder: true
  apply_to_text_encoder: true
  apply_to_detr_encoder: true
  apply_to_detr_decoder: true

# Adjust training for more capacity
training:
  max_epochs: 30
  batch_size: 1                    # Smaller batch due to more parameters
  gradient_accumulation_steps: 8   # Larger accumulation

  # Lower learning rate for stability
  lr_scale: 0.05
  lr_transformer: 4e-5
  lr_vision_backbone: 1.25e-5
  lr_language_backbone: 2.5e-6

  val_epoch_freq: 10
  checkpoint_freq: 5

launcher:
  gpus_per_node: 1                 # Or 2 if available
