# @package _global_
# Minimal LoRA Configuration for SAM3
# Most efficient - only decoder
# AI Research Group - KMUTT

defaults:
  - lora_base
  - _self_

# Override LoRA settings for minimal configuration
lora:
  rank: 4                          # Low rank for efficiency
  alpha: 8
  dropout: 0.0

  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"                     # Only attention, no MLP

  # Apply ONLY to decoder
  apply_to_vision_encoder: false   # Skip vision backbone
  apply_to_text_encoder: false     # Skip text encoder
  apply_to_detr_encoder: false     # Skip DETR encoder
  apply_to_detr_decoder: true      # ONLY decoder

# Adjust training for faster iteration
training:
  max_epochs: 100
  batch_size: 2
  gradient_accumulation_steps: 2

  # Higher learning rate for smaller model
  lr_scale: 0.2
  lr_transformer: 1.6e-4

  val_epoch_freq: 2
  checkpoint_freq: 2

launcher:
  gpus_per_node: 1
